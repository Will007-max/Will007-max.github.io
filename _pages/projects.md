---
layout: archive
tilte: 'My Projects'
permalink: /projects/
author_profile: true

comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

Currently, I am realizing several machine learning projects, the main is focused on deep learning for processing audio and videos in order to determine emotions in communication.

The analysis of emotions in a video streaming can be done by exploiting two channels: audio streaming, and the image sequence. In a first step, I only focused on audio.

Before the introduction of *transformers*, *MFCC coefficients* have been the state of the art in audio natural language processing. Research continues to be conducted for a better exploitation of audio to improve the applications of artificial intelligence in everyday life. Considering this point, I found it appropriate to use these two approaches to extract interesting features to feed to machine learning algorithms for prediction.

## I. Working with MFCC features using Librosa

Many Python librairies deal with signal and audio processing, here I only talk about Librosa.

For a better understanding, I would start by reminding some concepts upstream of the MFCC coefficients.

### I.1. Audio representations

#### I.1.1. Audio wave form

<%@ Language="Python" %>

<%

print('Hello world') 

%>

## II. Audio features with Wav2Vec2
