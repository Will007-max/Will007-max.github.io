---
title: About Spark
layout: archive
header:
    teaser: /assets/images/spark.png
author_profile: true
categories: tech_blog
read_time: true
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

## 1. Introduction to Spark

Spark is a popular solution on the market, and overcomes most of the limitations
of Hadoop MapReduce: results are written to disk resulting in longer execution
times, and it is difficult to express complex operations with Hadoop MapReduce
because it is limited and not very expressive.

Spark is able of interfacing with many data sources, as shown in the following:

![Image](/assets/images/spark_data_sources.png#right)

For instance, it can interface with MySQL and PostgreSQL data, on Hadoop HDFS,
there is also NoSQL, the injection with Kafka, the cached databases with Redis...

It is a universal tool that brings all the connectivity at the level of data
recovery, then it allows to distribute the calculations in an efficient way:it
tries to take advantage of all the existing resources, with the scalability that
is taken into account when there are new resources available.

It has modules to do just about anything: SQL queries, machine learning algorithms,
graph processing, stream processing (for instance data from sensors)...

## 2. Spark architecture

Just like MapReduce that doesn't play well with large scale tasks, the main
reason why Spark is useful is that it leverages **task parallelisation** on
**multiple** workers, and it is runs faster, and is more efficiently managed.

The architecture of Spark is represented in the following figure.

![Image](/assets/images/spark_architecture.png#right)

Globally, what makes Spark work is the Spark Core which is the software part on top

- A **cluster manager** deals with the resource allocation to get the tasks done

- Spark is based on a **master-slave architecture** i.e. the master coordinates
the workers to execute the tasks simultaneously.

- Spark is **faster** than Hadoop because Spark does the processing in the
**main memory** of the worker nodes and prevents unnecessary operations with the
disks

- The two mechanisms implemented in Spark to deal with failure are fault tolerance
and high availability. **Fault tolerance** is the ability for Spark to
**restart a job when the machine fails**, it keeps track of the work and is able
to go back at a certain point. **High availability** is the ability for a system
to **keep working** after one or more components fail

- Spark works with **A different API**: via Scala (native), Java, or Python
(PySpark), R. In terms of performance, Scala is about **10 times faster** than Python

- To import and launch a Spark session, you just need to write this line of code:

```python

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

```

- To load a CSV file, you may use the **spark.read.csv()** method

- The different datatypes in MLLib are DataFrame and RDD, Vector, LabeledPoint,
Local matrix and Distributed matrix (only in Java and Scala)

- The two reasons why PySpark is useful for data scientists are: it enables
scalable analysis, ML pipelines. It is also easy to work with because of its
familiarities with Scikit-learn and Pandas. The main drawback of Scikit-learn and
Pandas when building predictive models is that Scikit-learn **doesn't scale to**
**large datasets** in a distributed environment (no task parallelisation)

- To convert a PySpark dataframe into a Pandas one, you can use the **toPandas()**
method. Be careful when using it because all data are **loaded into memory** on
the driver node and prevents operations from being performed in a **distributed mode**

- Before the training of your algorithm in MLlib, you need to create a **vector**
**representation** of the features that you'll use, as in the following:

```python

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=['col1', 'col2', 'col3'],
                            outputCol='features')
train_df = assembler.transform(df)

```

- It is easy to make predictions on a test set:

```python

pipeline = Pipeline(stages=[vector_assembler, classifier])

model = pipeline.fit(train)

predictions = model.transform(test)

```

- You can combine multiple algorithms (transformers and an estimator) into a
single pipeline with **SparkML**
