---
title: About Spark
layout: single
header:
    teaser: /assets/images/spark.png
author_profile: true
categories: tech_blog
read_time: true
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

## 1. Introduction to Spark

Spark is a popular solution on the market, and overcomes most of the limitations
of Hadoop MapReduce (results are written to disk resulting in longer execution
times, and it is difficult to express complex operations with Hadoop MapReduce
because it is limited and not very expressive).

Spark is able of interfacing with many data sources, as shown in the following:

![Image](/assets/images/spark_data_sources.png#right)

For instance, it can interface with MySQL and PostgreSQL data, on Hadoop HDFS,
there is also NoSQL, the injection with Kafka, the cached databases with Redis...

It is a universal tool that brings all the connectivity at the level of data
recovery, then it allows to distribute the calculations in an efficient way: it
tries to take advantage of all the existing resources, with the scalability that
is taken into account when there are new resources available.

It has modules to do just about anything: SQL queries, machine learning algorithms,
graph processing, stream processing (for instance data from sensors)...

## 2. Spark architecture

Just like MapReduce that doesn't play well with large scale tasks, the main
reason why Spark is useful is that it leverages **task parallelisation** on
**multiple** workers, and it is runs faster, and is more efficiently managed.

The architecture of Spark is represented in the following figure.

![Image](/assets/images/spark_architecture.png#right)

Globally, what makes Spark work is the **Spark Core** which is the software part
on top.

Just below there is the group of possibilities to run Spark (at the hardware
level): either on your local machine (Spark standalone), or there is the possibility to manage this hardware part on a cluster using a **cluster manager** for resources
allocation: there is YARN (Yet Another Resource Negotiator) introduced with
Hadoop, MESOS which is an Apache project, and even Kubernetes...

And at the very bottom, there is the layer representing where the data is located.

Spark works with **4 different API**: via Scala (native), Java, or Python
(PySpark), R. In terms of performance, Scala is about **10 times faster** than
Python.

## 3. Spark cluster, drivers and workers, Resilient Distributed Dataset

Spark is based on a **master-slave architecture** (cf the following figure) i.e.
the master coordinates the workers to execute the tasks simultaneously.

![Image](/assets/images/spark_cluster.png#right)

Before continuing, I would like to mention that driver, master, name node mean
the same thing. It is like worker, slave and data node that are the same thing.

When Spark is used, it starts by communicating with the master machine, a Spark
entry point to this machine is the SparkContext (or SparkSession). To import and
launch a Spark session, you just need to write this line of code:

```python

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

```

In the back it communicates with a **cluster manager**. And it's the latter that
deals with the resource allocation to get the tasks done. It will choose which computations to allocate to which machines and at which quantities.

Within the cluster, there are workers that have a ***cache*** part to store data,
and an ***execution*** part to perform ***tasks*** on data available on these
nodes. Spark is therefore **faster** than Hadoop since Spark does the
processing in the **main memory** of the worker nodes and prevents unnecessary
operations with the disks.

## 4. Fault tolerance - High availability - Resilient Distributed Dataset

Hadoop MapReduce is known for data storage on disks. Considering the fact that
the Spark data is stored in a cache, we can ask ourselves the question of whether something is lost in terms of guarantee in case a node crashes or a job does not run.

In fact, there are two mechanisms implemented in Spark to deal with failure: fault tolerance and high availability.

**Fault tolerance** is the ability for Spark to **restart a job when the machine fails**, it keeps track of the job that was running and is able to go back at a certain point, and not starting from scratch. It is able to restart the job and guarantee the result behind it.

**High availability** is the ability for a system to **keep working** after one
or more components fail. If it's a software part that crashes, the job is lost
and it is re-run somewhere else, with a small delay to get the result.

In both cases, what happens is that the job is re-launched afterwards.



- To load a CSV file, you may use the **spark.read.csv()** method

- The different datatypes in MLLib are DataFrame and RDD, Vector, LabeledPoint,
Local matrix and Distributed matrix (only in Java and Scala)

- The two reasons why PySpark is useful for data scientists are: it enables
scalable analysis, ML pipelines. It is also easy to work with because of its
familiarities with Scikit-learn and Pandas. The main drawback of Scikit-learn and
Pandas when building predictive models is that Scikit-learn **doesn't scale to**
**large datasets** in a distributed environment (no task parallelisation)

- To convert a PySpark dataframe into a Pandas one, you can use the **toPandas()**
method. Be careful when using it because all data are **loaded into memory** on
the driver node and prevents operations from being performed in a **distributed mode**

- Before the training of your algorithm in MLlib, you need to create a **vector**
**representation** of the features that you'll use, as in the following:

```python

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=['col1', 'col2', 'col3'],
                            outputCol='features')
train_df = assembler.transform(df)

```

- It is easy to make predictions on a test set:

```python

pipeline = Pipeline(stages=[vector_assembler, classifier])

model = pipeline.fit(train)

predictions = model.transform(test)

```

- You can combine multiple algorithms (transformers and an estimator) into a
single pipeline with **SparkML**
